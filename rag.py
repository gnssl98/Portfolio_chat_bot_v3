import os
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableLambda
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from transformers import BertTokenizer, BertForSequenceClassification, MarianTokenizer, MarianMTModel
from langchain.prompts import PromptTemplate
from openai import OpenAI
from typing import TypedDict
import torch
from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

load_dotenv()

# ====== ê²½ë¡œ ì„¤ì • ======
VECTOR_PATH = "./vector_store"
KO_EN_MODEL_PATH = r"D:\\dataset\\fine_tuned_model\\translation_model\\ko_en_finetuned"
EN_KO_MODEL_PATH = r"D:\\dataset\\fine_tuned_model\\translation_model\\en_ko_finetuned"
QA_MODEL_PATH = r"D:\\dataset\\fine_tuned_model\\flan-t5-large"

# ====== ë””ë°”ì´ìŠ¤ ì„¤ì • ======
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ====== ë²¡í„° ì €ì¥ì†Œ ë¡œë”© ======
embedding = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
vectorstore = FAISS.load_local(VECTOR_PATH, embedding, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever()

# ====== QA ëª¨ë¸ ë¡œë”© ======
qa_tokenizer = AutoTokenizer.from_pretrained(QA_MODEL_PATH)
qa_model = AutoModelForSeq2SeqLM.from_pretrained(QA_MODEL_PATH).to(device)
qa_pipeline = pipeline("text2text-generation", model=qa_model, tokenizer=qa_tokenizer, max_new_tokens=128, device=0 if torch.cuda.is_available() else -1)
qa_chain = HuggingFacePipeline(pipeline=qa_pipeline)

# ====== ë²ˆì—­ ëª¨ë¸ ë¡œë”© ======
ko_en_tokenizer = MarianTokenizer.from_pretrained(KO_EN_MODEL_PATH)
ko_en_model = MarianMTModel.from_pretrained(KO_EN_MODEL_PATH).to(device)

en_ko_tokenizer = MBart50TokenizerFast.from_pretrained(EN_KO_MODEL_PATH)
en_ko_model = MBartForConditionalGeneration.from_pretrained(EN_KO_MODEL_PATH).to(device)
en_ko_tokenizer.src_lang = "en_XX"

# ====== ë²ˆì—­ í•¨ìˆ˜ ì •ì˜ ======
def translate_marian(text, tokenizer, model):
    inputs = tokenizer([text], return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=128,
            num_beams=5,
            no_repeat_ngram_size=2
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def translate_mbart(text, tokenizer, model, src_lang="en_XX", tgt_lang="ko_KR"):
    tokenizer.src_lang = src_lang
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],
            max_length=128,
            num_beams=5,
            no_repeat_ngram_size=2
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ====== GPT API í´ë¼ì´ì–¸íŠ¸ ======
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ====== ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ======
clf_tokenizer = BertTokenizer.from_pretrained("./classifier_model")
clf_model = BertForSequenceClassification.from_pretrained("./classifier_model")
clf_model.eval().cuda()

# ====== ìƒíƒœ ì •ì˜ ======
class PortfolioState(TypedDict, total=False):
    question: str
    is_portfolio: bool
    context: str
    answer: str

# ====== ë…¸ë“œ ì •ì˜ ======
def classify_with_model(state):
    question = state["question"]
    inputs = clf_tokenizer(question, return_tensors="pt", truncation=True, padding=True).to("cuda")
    with torch.no_grad():
        logits = clf_model(**inputs).logits
        is_portfolio = torch.argmax(logits, dim=-1).item() == 1
    print("[LOG] classify_with_model ì‹¤í–‰ë¨ â†’ is_portfolio:", is_portfolio)
    print("[LOG] ë°˜í™˜ state:", {**state, "is_portfolio": is_portfolio})
    return {**state, "is_portfolio": is_portfolio}

def retrieve_context_node(state):
    print("[LOG] retrieve_context_node ì‹¤í–‰ë¨ / state:", state)
    try:
        docs = retriever.get_relevant_documents(state["question"])
        print("[LOG] ê´€ë ¨ ë¬¸ì„œ ê°œìˆ˜:", len(docs))
        context = "\n".join([doc.page_content for doc in docs])
        context = context.replace("\n", " ").strip()[:1000]  # ğŸ§¹ ì „ì²˜ë¦¬: ì¤„ë°”ê¿ˆ ì œê±° + ê¸¸ì´ ì œí•œ
        print("[LOG] context ë‚´ìš©:", context)
        return {**state, "context": context}
    except Exception as e:
        print("[ERROR] retrieve_context_node ì˜ˆì™¸:", e)
        return {**state, "context": ""}

def run_qa_pipeline_node(state):
    print("[LOG] run_qa_pipeline_node ì‹¤í–‰ë¨ / context ì¡´ì¬ ì—¬ë¶€:", bool(state.get("context")))
    question_ko = state["question"]
    context_ko = state["context"]
    question_en = translate_marian(question_ko, ko_en_tokenizer, ko_en_model)
    context_en = translate_marian(context_ko, ko_en_tokenizer, ko_en_model)

    prompt = PromptTemplate.from_template("""
    You are a professional assistant that answers interview questions based on the applicant's portfolio.

    Below is the applicant's self-introduction document:
    -----------
    {context}
    -----------
    Now answer the following interview question in a concise and professional manner:
    Q: {question}
    """)
    final_prompt = prompt.format(question=question_en, context=context_en)
    answer_en = qa_chain.invoke(final_prompt)

    if isinstance(answer_en, dict):
        answer_en = answer_en.get("generated_text", "")

    print("[LOG] ì˜ì–´ ë‹µë³€:", answer_en)

    answer_ko = translate_mbart(answer_en, en_ko_tokenizer, en_ko_model, src_lang="en_XX", tgt_lang="ko_KR")
    print("[LOG] ìµœì¢… ë²ˆì—­ëœ ë‹µë³€:", answer_ko)

    return {**state, "answer": answer_ko}

def run_gpt_fallback_node(state):
    print("[LOG] run_gpt_fallback_node ì‹¤í–‰ë¨")
    question = state["question"]
    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•œ ì±—ë´‡ì´ì•¼."},
        {"role": "user", "content": question}
    ]
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages
    )
    return {**state, "answer": response.choices[0].message.content.strip()}

# ====== ë¶„ê¸° ì¡°ê±´ í•¨ìˆ˜ ======
def route_classify(state: PortfolioState) -> str:
    print("[DEBUG] ë¶„ê¸° ì¡°ê±´ í‰ê°€ ì¤‘ - is_portfolio:", state.get("is_portfolio"))
    return "retrieve" if state.get("is_portfolio", False) else "gpt_fallback"

# ====== LangGraph êµ¬ì„± ======
def build_portfolio_graph():
    graph = StateGraph(state_schema=PortfolioState)

    graph.add_node("classify", RunnableLambda(classify_with_model))
    print("[DEBUG] classify ë…¸ë“œ ë“±ë¡")
    graph.add_node("retrieve", RunnableLambda(retrieve_context_node))
    print("[DEBUG] retrieve ë…¸ë“œ ë“±ë¡")
    graph.add_node("qa", RunnableLambda(run_qa_pipeline_node))
    print("[DEBUG] qa ë…¸ë“œ ë“±ë¡")
    graph.add_node("gpt_fallback", RunnableLambda(run_gpt_fallback_node))
    print("[DEBUG] gpt_fallback ë…¸ë“œ ë“±ë¡")

    graph.set_entry_point("classify")
    print("[DEBUG] entry point ì„¤ì •")

    graph.add_conditional_edges("classify", route_classify)
    print("[DEBUG] ë¶„ê¸° ì¡°ê±´ í•¨ìˆ˜ ì ìš© ì™„ë£Œ")

    graph.add_edge("retrieve", "qa")
    graph.add_edge("qa", END)
    graph.add_edge("gpt_fallback", END)
    print("[DEBUG] ê·¸ë˜í”„ ì—£ì§€ ì„¤ì • ì™„ë£Œ")

    return graph.compile()

# ====== ì‹¤í–‰ ======
dialogue_graph = build_portfolio_graph()

if __name__ == "__main__":
    user_input = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    try:
        result = dialogue_graph.invoke({"question": user_input})
        print("[DEBUG] invoke ë°˜í™˜ê°’:", result)
        if "answer" in result and result["answer"]:
            print("\n ë‹µë³€:", result["answer"])
        else:
            print("\n ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("[ERROR] ì „ì²´ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e)
